Rate Limiter

 A rate limiter is used to control the rate of traffic sent by a client or a service. In the HTTP world, a rate limiter limits the number of client requests allowed to be sent over a specified period. 

If the API request count exceeds the threshold defined by the rate limiter, all the excess calls are blocked. Here are a few examples:
• A user can write no more than 2 posts per second.
• You can create a maximum of 10 accounts per day from the same IP address.
• You can claim rewards no more than 5 times per week from the same device.

	▪	Functional Requirements	
	⁃	 Server-side Rate Limiter
	⁃	Rate limiter should be flexible to throttle based on IP address/user ID/all the properties.
	⁃	It should support up to 1,000,000 request.
	⁃	System will work in distributed environment.
	⁃	Users should get notified about throttled request.

	▪	Non-Functional Requirements 
	⁃	low latency.
	⁃	use of limited memory.
	⁃	High fault tolerance : if there is any problem with rate limit, it should not affect the entire system.

	▪	Where to put the rate limit?
	⁃	rate limit can be inserted inside the API server, or we can have a separate middleware.

	▪	Algorithms for Rate Limiter:
	⁃	Token Bucket
	⁃	Leaking Bucket
	There are numerous algorithms are available, can use dependent on requirements. 

	▪	High-level Architecture of Rate Limiter
	The basic idea of rate limiting algorithms is simple. At the high-level, we need a counter to keep track of how many requests are sent from the same user, IP address, etc. If the counter is larger than the limit, the request is disallowed.

▪	Where to store the counters ?

Using the database is not a good idea due to slowness of disk access. In-memory cache is chosen because it is fast and supports time-based expiration strategy. 

For instance, Redis [11] is a popular option to implement rate limiting. It is an in-memory store that offers two commands: INCR and EXPIRE.
• INCR: It increases the stored counter by 1.
• EXPIRE: It sets a time


▪	How it Works ?
• The client sends a request to rate limiting middleware.
• Rate limiting middleware fetches the counter from the corresponding bucket in Redis andchecks if the limit is reached or not.
• If the limit is reached, the request is rejected.
• If the limit is not reached, the request is sent to API servers. Meanwhile, the system increments the counter and saves it back to Redis.


▪	How it Works for Distributed Environment ?
Building a rate limiter that works in a single server environment is not difficult. However, scaling the system to support multiple servers and concurrent threads is a different story.
There are two challenges:
• Race condition
• Synchronization issue





